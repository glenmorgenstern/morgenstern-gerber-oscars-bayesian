---
title: "STA 360 Final Project"
author: "Glen Morgenstern and Jason Gerber"
date: "4/24/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(broom)
require(rstanarm)
require(magrittr)
library(ggplot2)
require(loo)
require(bayesplot)
require(caret)
library(rstan)
require(HSAUR3)
```

```{r}
oscars <- read_csv("data/oscars.csv")
director_noms <- oscars %>%
  filter(DD == 1, Year != 2006) %>%
  rename(win = Ch) %>%
  mutate(win = ifelse(win==1, 1, 0))
# win is 1 if a win, 0 if loss
```

# Introduction: A few paragraphs which (i) motivate problem importance & relevance

  Every year, billions of people around the globe watch movies, and millions tune in to watch the Academy Awards. This awards show is heavily publicized, promoted, and talked about every year, where millions of fans tune in to watch the ceremony and tens of millions(perhaps more) read about it later. Receiving an Oscar is the highest honor that one can procure in the business of movie-making, and since movies are such a large industry and many of our country's most influential celebrities are movie stars, the Oscars is one of the biggest events there is each year in the United States. All this attention inevitably leads to general curiosity and predictions about who will take home the most prized awards in all of filmmaking, which is where the reason for this project arises.
  
  The goal of this project is to develop a model which will predict with as much accuracy as possible the winner of the "Best Director" category at the Oscars. Being able to predict Oscar winners is an analysis which many across the globe will have an interest in, whether they be data-lovers, typical cinephiles, or even gamblers who feel that they need an edge. 
  
  To achieve our goal, we will take several steps. First, we will perform exploratory data analysis on the data set we have in order to discover which variables have the potential to be explanatory variables for the model. Second, we will establish a clear prior for the model. Third, we will determine which explanatory variables should be used for the model by combining them with the prior to form a posterior, by creating several Bayesian logistical regression models and testing until we reach a model with optimal fit. Lastly, we will interpret the model coefficients and discuss its applications.

# Data: A couple of paragraphs describing the data to be used. You may wish to

  The data set that we are using for this analysis is one that contains information about all Oscar nominations from 1928-2006. The data has been taken from the personal website of Iain Pardoe, an online educator in mathematics and statistics, in the form of a csv, which we read into R and made into a data frame. 
  
  In our model, we will attempt to use all relevant variables that may be significant predictors of whether a director wins best director or not. The data set given has 62 variables, of which one is an indicator variable specifying if the nominee is up for Best Director, and one of which is an indicator variable specifying if the nominee won their award or not. For example, if Martin Scorsese is nominated for best director, the variable DD(director nominee indicator) will equal 1. We will create our data set using only the instances where DD is equal to 1, as we are only focusing on predicting winners among nominated directors.
  
  To decide which variables to use, we performed EDA on many variables, but since we have limited space, we are only discussing the variables relevant to our model. In other words, we did not perform EDA on variables like AD (Whether the Assistant Director was nominated) because it simply did not show any correlation with the director winning Best Director. This was true for all built-in interaction variables as well.

  The variables that we performed EDA on are described below:
    
    Nom: The total number of oscar nominations that the director's movie received.
    
    Pic: An indicator variable that specifies if the director's movie was also nominated for best picture.
    
    Aml and Afl: Indicator variables that specify if the lead actor or actress in the director's movie was nominated for best lead actor or actress, respectively.
    
    Ams and Afs: Indicator variables that specify if the supporting actor or actress in the director's movie was nominated for best supporting actor or actress, respectively.
    
    Scr: An indicator variable that specifies if the director's movie was also nominated for best screenplay.
    
    Cin: An indicator variable that specifies if the director's movie was also nominated for best cinematography.
    
    Art: An indicator variable that specifies if the director's movie was also nominated for best art direction.
    
    Cos: An indicator variable that specifies if the director's movie was also nominated for best costumes.
    
    Sco: An indicator variable that specifies if the director's movie was also nominated for best muscial score.
    
    Edi: An indicator variable that specifies if the director's movie was also nominated for best editing.
    
    Sou: An indicator variable that specifies if the director's movie was also nominated for best sound mixing.
    
    Eff: An indicator variable that specifies if the director's movie was also nominated for best special effects.
    
    PrN: The total number of oscar nominations that the director and actors for the movie had received previously.
    
    PrW: The total number of oscars that the director and actors for the movie had won previously.
    
    Gd: An indicator variable that specifies if the director's movie won a Golden Globe for Directing earlier in the same award season.
    
    DGA: An indicator variable that specifies if the director won an award from the Director's Guild of America earlier in the same award season.
    
    Year: The Year that the movie was nominated for an Oscar.



  
## EDA

```{r}
ggplot(data = director_noms, aes(x = Nom, fill = as.character(win))) + geom_histogram(binwidth = 1) + 
  labs(title = "Total Oscar Nominations", x = "Total Nominations")  + 
  scale_fill_manual(name = "Won", values = c("blue", "red"))
```
  
```{r}
#Total nominations matter a lot
director_noms %>%
  group_by(win) %>%
  summarise(mean_noms = mean(Nom),
            Q1_noms = quantile(Nom, c(0.25)),
            Q3_noms = quantile(Nom, c(0.75)))

# Best picture matters a lot
director_noms %>%
  group_by(win) %>%
  summarise(total = n(), total_best_picture = length(Pic[Pic == 1]),
            prop_picture = total_best_picture/total)

# Male leads matter a lot 
director_noms %>%
  group_by(win) %>%
  summarise(total = n(), prop_at_least_1_male_lead = length(Aml[Aml>=1])/n(),
            std_dev_male_lead = sd(Aml))

# Female leads matter less 
director_noms %>%
  group_by(win) %>%
  summarise(total = n(), prop_at_least_1_female_lead = length(Afl[Afl>=1])/n(),
            std_dev_male_lead = sd(Afl))

# Difference in both male and female supporting actors
director_noms %>%
  group_by(win) %>%
  summarise(prop_at_least_1_male_support = length(Ams[Ams>=1])/n(),
            std_dev_male_support= sd(Ams))

director_noms %>%
  group_by(win) %>%
  summarise(prop_at_least_1_female_support = length(Afs[Afs>=1])/n(),
            std_dev_male_lead = sd(Afs))


```

### More EDA

```{r}
director_noms %>%
  group_by(win) %>%
  summarise(prop_screenplay_nom = length(Scr[Scr==1])/n())

director_noms %>%
  group_by(win) %>%
  summarise(prop_cinematography_nom = length(Cin[Cin==1])/n())

director_noms %>%
  group_by(win) %>%
  summarise(prop_art_nom = length(Art[Art==1])/n())

director_noms %>%
  group_by(win) %>%
  summarise(prop_costum_nom = length(Cos[Cos==1])/n())

director_noms %>%
  group_by(win) %>%
  summarise(prop_score_nom = length(Sco[Sco==1])/n())

# All appear to be somewhat relevant??
```


```{r}
# double the chance
director_noms %>%
  group_by(win) %>%
  summarise(prop_score_nom = length(Edi[Edi==1])/n())

# 2.5x the chance
director_noms %>%
  group_by(win) %>%
  summarise(prop_score_nom = length(Sou[Sou==1])/n())

# double chance, but not too many are nominated
director_noms %>%
  group_by(win) %>%
  summarise(prop_score_nom = length(Eff[Eff==1])/n())
```


```{r}
# Slight Advantage to Nominees
director_noms %>%
  group_by(win) %>%
  summarise(prop_score_nom = mean(PrN))

# Slight Advantage to Winners
director_noms %>%
  group_by(win) %>%
  summarise(prop_score_nom = mean(PrW))

```


```{r}
# previous Wins

# Extreme Correlation
director_noms %>%
  group_by(win) %>%
  summarise(prop_gd_win = length(Gd[Gd == 1])/n())

# Extreme Correlation, if they win the DGA, choose them to win the Oscar
director_noms %>%
  group_by(win) %>%
  summarise(prop_DGA = length(DGA[DGA == 1])/n())


```

# Check for real interactions!!

```{r}
# High correlation interaction variables!!

# year interactions

director_wins <- director_noms %>%
  filter(win == 1)

director_wins %>%
  summarise(mean(Art*Sco))

```

```{r}
# other variables that I tested but were not significant at all: PrN, PrW, Sou, Sco, Cos, Art, Aml, Afl, Ams, Afs

log.model <- glm(win ~ Nom + Pic + Scr + Cin + Edi + Eff +
                   Gd + DGA + Year
                 , data = director_noms, family = binomial)

tidy(log.model, exponentiate = FALSE, conf.int = TRUE)


```
## How do I check if a logistic model fits??




## Data sources clearly described
  

# Model: Discussion & justification of the proposed Bayesian model framework
(prior and sampling model). This discussion should elicit prior information on the
problem, the data sources available, and relevant project goals. Any “downstream”
uses of the model (e.g., for prediction, optimization, ranking) should be discussed
in detail here. See project rubric for details.


```{r}
post1 <- stan_glm(win ~ Nom + Pic + Scr + Cin + Edi + Eff +
                   Gd + DGA + Year, data = director_noms,
                 family = binomial(link = "logit"), 
                 prior = normal(0,10), prior_intercept = normal(0,1),
                 seed = 196,
                 refresh = 0)
```

```{r, eval = F}
launch_shinystan(post1)
```

Now we can look at posterior densities and estimates for the coefficients.

```{r, cache = T}
mcmc_areas(as.matrix(post1), prob = 0.95, prob_outer = 1)
round(coef(post1), 3)
round(posterior_interval(post1, prob = 0.95), 3)

```



```{r}
# Got rid of Nom, Gd, Eff 
post2 <- stan_glm(win ~ Pic + Scr + Edi + Cin +
                    DGA + Year, data = director_noms,
                 family = binomial(link = "logit"), 
                 prior = normal(0,10), prior_intercept = normal(0,1),
                 seed = 196,
                 refresh = 0)
```

```{r}
mcmc_areas(as.matrix(post2), prob = 0.95, prob_outer = 1)
round(coef(post2), 3)
round(posterior_interval(post2, prob = 0.95), 3)
```

```{r}
(loo2 <- loo(post2, save_psis = TRUE))
```

```{r}
post3 <- stan_glm(win ~ Nom + Pic + Aml + Afl + Ams + Afs + Scr + Cin + Edi +
                    Eff + Gd + DGA + Year, data = director_noms,
                 family = binomial(link = "logit"), 
                 prior = normal(0,1), prior_intercept = normal(0,1),
                 seed = 196,
                 refresh = 0)

mcmc_areas(as.matrix(post3), prob = 0.95, prob_outer = 1)
round(coef(post3), 3)
round(posterior_interval(post3, prob = 0.95), 3)
```

```{r}
(loo3 <- loo(post3, save_psis = TRUE))
```

# Results: Posterior analyses from the fitted Bayesian model, and a translation of
such findings into meaningful & understandable conclusions for the target audience
(e.g., engineers, business managers, policy-makers, etc). See project rubric for
details.

```{r, cache = T}
(loo1 <- loo(post1, save_psis = TRUE))
```

In the code chunk above, we assessed the strength of our model via its posterior preditive LOOCV. However as we know, this accuracy rate is quite meaningless unless we have something to compare it to. So let's create a baseline model with no predictors to compare to this first model:

```{r, cache = T}
post0 <- stan_glm(win ~ 1, data = director_noms,
                 family = binomial(link = "logit"), 
                 prior = normal(0,1), prior_intercept = normal(0,1),
                 seed = 196,
                 refresh = 0)
(loo0 <- loo(post0, save_psis = T))
rstanarm::compare_models(loo1, loo2)
```

# Conclusion: A summary of key findings and potential impacts of your project.
