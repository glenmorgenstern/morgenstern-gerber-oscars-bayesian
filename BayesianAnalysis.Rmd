---
title: "STA 360 Final Project"
author: "Glen Morgenstern and Jason Gerber"
date: "4/24/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(broom)
```

```{r}
oscars <- read_csv("data/oscars.csv")
director_noms <- oscars %>%
  filter(DD == 1, Year != 2006) %>%
  rename(win = Ch) %>%
  mutate(win = ifelse(win==1, 1, 0))
# win is 1 if a win, 0 if loss
```

# trouble with viewing win column??

# Introduction: A few paragraphs which (i) motivate problem importance & relevance
(supported by any pertinent literature), (ii) describe project goals and how such
goals address the problem, (iii) a high-level roadmap of the proposed Bayesian
modeling framework, and (iv) other relevant information for the reader. See project
rubric for details.

  Billions of people around the globe watch movies, and millions tune in to watch the Academy Awards every year(citation). 
  
  The goal of this project is to create a model which will predict with as much accuracy as possible the four major category winners at the Oscars. The Academy Awards is heavily publicized, promoted, and talked about every year, where millions of fans tune in to watch the ceremony and tens of millions(perhaps more) read about it later. Receiving an Oscar is the highest honor that one can procure in the business of movie-making, and since movies are such a large industry and many of our country's most influential celebrities are movie stars, the Oscars is one of the biggest events there is each year in the United States.
  
  All this attention inevitably leads to general curiosity and predictions about who will take home the most prized awards in all of filmmaking, which is where the reason for this project arises. 
  
  The goal of this project is to create a model which will predict with as much accuracy as possible the four major category winners at the Oscars, which include Best Lead Actor, Best Lead Actress, Best Picture, and Best Director. Being able to predict Oscar winners is an analysis which many across the globe will have an interest in, whether they be data-lovers, typical cinephiles, or even gamblers who feel that they need an edge. 
  
  ## high level roadmap of proposed modeling framework.
  
  ## reader info
  
  ## fix up wording here
  
  • Data sources are clearly outlined & fully described. If needed, a clear procedure is given on how to get from raw data to usable data.
• All variables clearly identified and described for the study. Any relevant features of these variables (e.g., data type, interpretation, etc.) should be fully discussed.
• The conclusions obtained from the proposed data can fully address project goals. If not, potential limitations in the data should be discussed fully.
• Project goals are addressed in a comprehensive and nuanced way. 
• Exploratory data analysis should support project goals and help guide specification of model.


  
  
  

# Data: A couple of paragraphs describing the data to be used. You may wish to
discuss: (i) data sources – where are you getting the data? (ii) data description –
what data / variables will be used for modeling? (iii) data type – ordinal discrete,
nominal discrete, continuous, etc., and (iv) data scraping / wrangling – how to
extract and clean data for modeling? See project rubric for details.

  The data set that we are using for this analysis is one that contains information about all Oscar nominations from 1928-2006 for the four major academy awards: Best Picture, Best Director, Best Leading Actor, and Best Leading Actress. The data has been taken from the personal website of Iain Pardoe, an online educator in mathematics and statistics. 
  
  In our model, we will attempt to use all relevant variables that may be significant predictors of whether a candidate wins their award type or not. The data set given has 62 variables, of which 4 are indicator variables indicating if the nominee is up for that award. For example, if Kate Winslet is nominated for best leading actress, the variable FF(lead actress indicator) will equal 1, while the indicator variables for lead actor, best picture, and best director will equal 0. These variables will help us separate to make four separate models, one for predicting each award type,
  
  Our explanatory variables include the year the movie was produced, as well as several statistics involving other people who worked on the movie that the award nominee is from. For example, there is an indicator variable that specifies if the editing in the movie was nominated for an Oscar, a number of times the lead actor for the movie was previously nominated, and more similar variables. The basic idea is to predict if the nominee will win the award based on the strength of the rest of the movie and its cast. 
  
  Additionally, there are several interaction variables already built into the data
  ### Glen, idk if we want to add MORE interactions?
  
  ## Limitations

## Project Goals
  
## EDA

```{r}
ggplot(data = director_noms, aes(x = Nom, fill = as.character(win))) + geom_histogram(binwidth = 1) + 
  labs(title = "Total Oscar Nominations", x = "Total Nominations")  + 
  scale_fill_manual(name = "Won", values = c("blue", "red"))
```
  
```{r}
#Total nominations matter a lot RELEVANT
director_noms %>%
  group_by(win) %>%
  summarise(mean_noms = mean(Nom),
            Q1_noms = quantile(Nom, c(0.25)),
            Q3_noms = quantile(Nom, c(0.75)))

# Best picture matters a lot RELEVANT
director_noms %>%
  group_by(win) %>%
  summarise(total = n(), total_best_picture = length(Pic[Pic == 1]),
            prop_picture = total_best_picture/total)

# Male leads matter a lot RELEVANT
director_noms %>%
  group_by(win) %>%
  summarise(total = n(), prop_at_least_1_male_lead = length(Aml[Aml>=1])/n(),
            std_dev_male_lead = sd(Aml))

# Female leads matter less NOT RELEVANT
director_noms %>%
  group_by(win) %>%
  summarise(total = n(), prop_at_least_1_female_lead = length(Afl[Afl>=1])/n(),
            std_dev_male_lead = sd(Afl))

# Difference in both male and female supporting actors RELEVANT
director_noms %>%
  group_by(win) %>%
  summarise(prop_at_least_1_male_support = length(Ams[Ams>=1])/n(),
            std_dev_male_support= sd(Ams))

director_noms %>% # NOT RELEVANT
  group_by(win) %>%
  summarise(prop_at_least_1_female_support = length(Afs[Afs>=1])/n(),
            std_dev_male_lead = sd(Afs))


```

### More EDA

```{r}
director_noms %>%
  group_by(win) %>%
  summarise(prop_screenplay_nom = length(Scr[Scr==1])/n())

director_noms %>%
  group_by(win) %>%
  summarise(prop_cinematography_nom = length(Cin[Cin==1])/n())

director_noms %>%
  group_by(win) %>%
  summarise(prop_art_nom = length(Art[Art==1])/n())

director_noms %>%
  group_by(win) %>%
  summarise(prop_costum_nom = length(Cos[Cos==1])/n())

director_noms %>%
  group_by(win) %>%
  summarise(prop_score_nom = length(Sco[Sco==1])/n())

# All appear to be somewhat relevant??
```


```{r}
# double the chance
director_noms %>%
  group_by(win) %>%
  summarise(prop_score_nom = length(Edi[Edi==1])/n())

# 2.5x the chance
director_noms %>%
  group_by(win) %>%
  summarise(prop_score_nom = length(Sou[Sou==1])/n())

# double chance, but not too many are nominated
director_noms %>%
  group_by(win) %>%
  summarise(prop_score_nom = length(Eff[Eff==1])/n())
```


```{r}
# Slight Advantage to Nominees
director_noms %>%
  group_by(win) %>%
  summarise(prop_score_nom = mean(PrN))

# Slight Advantage to Winners
director_noms %>%
  group_by(win) %>%
  summarise(prop_score_nom = mean(PrW))

```


```{r}
# previous Wins

# Extreme Correlation
director_noms %>%
  group_by(win) %>%
  summarise(prop_gd_win = length(Gd[Gd == 1])/n())

# Extreme Correlation, if they win the DGA, choose them to win the Oscar
director_noms %>%
  group_by(win) %>%
  summarise(prop_DGA = length(DGA[DGA == 1])/n())



```

# Check for real interactions!!

```{r}
# High correlation interaction variables!!

# year interactions

director_wins <- director_noms %>%
  filter(win == 1)

director_wins %>%
  summarise(mean(Art*Sco))

```

```{r}
# other variables that I tested but were not significant at all: PrN, PrW, Sou, Sco, Cos, Art, Aml, Afl, Ams, Afs

log.model <- glm(win ~ Nom + Pic + Scr + Cin + Edi + Eff +
                   Gd + DGA + Year
                 , data = director_noms, family = binomial)

tidy(log.model, exponentiate = FALSE, conf.int = TRUE)


```
## How do I check if a logistic model fits??




## Data sources clearly described
  

# Model: Discussion & justification of the proposed Bayesian model framework
(prior and sampling model). This discussion should elicit prior information on the
problem, the data sources available, and relevant project goals. Any “downstream”
uses of the model (e.g., for prediction, optimization, ranking) should be discussed
in detail here. See project rubric for details.


# Results: Posterior analyses from the fitted Bayesian model, and a translation of
such findings into meaningful & understandable conclusions for the target audience
(e.g., engineers, business managers, policy-makers, etc). See project rubric for
details.


# Conclusion: A summary of key findings and potential impacts of your project.
